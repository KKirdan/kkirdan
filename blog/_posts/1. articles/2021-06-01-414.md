---
layout: post
title: Искусственный интеллект и страдание
show_title: true
updated: 2024-06-19
unixtime:
  - 1622557140
  - 1718795094
vk: https://vk.com/wall-178968945_414
medium: https://medium.com/@k.kirdan/%D1%81%D0%B8%D0%BB%D1%8C%D0%BD%D1%8B%D0%B9-%D0%B8%D1%81%D0%BA%D1%83%D1%81%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9-%D0%B8%D0%BD%D1%82%D0%B5%D0%BB%D0%BB%D0%B5%D0%BA%D1%82-%D0%B8-%D1%83%D0%BC%D0%B5%D0%BD%D1%8C%D1%88%D0%B5%D0%BD%D0%B8%D0%B5-%D1%81%D1%82%D1%80%D0%B0%D0%B4%D0%B0%D0%BD%D0%B8%D0%B9-52cf7f1b10fa
tags:
  - искусственный_интеллект
  - глобальные_риски
  - счастье_и_несчастье
meta_tags: черновик
importance: 9
---
Как людям, желающим (главным образом) [уменьшения страданий во вселенной](https://reducingsuffering.github.io/what-is-suffering-reduction.html), относиться к разработке все более и более мощных форм искусственного интеллекта? ИИ может стать причиной многих страданий (в частности, сам их испытывать), или же напротив — может сыграть какую-то роль в их уменьшении. Особенное беспокойство вызывают идеи общего искусственного интеллекта (ОИИ) и/или сверхинтеллекта.

Я изложу здесь некоторые из своих соображений по этой теме и предоставлю ссылки для более глубокого изучения.

<b class="draft">Статья сейчас находится в процессе переписывания.</b>

## Содержание

- **[1. О чем идет речь](#1)**
- **[2. Когда ждать](#2)**
- **[3. Роль ценностей](#3)**
  - **[3.1. Кто у руля](#3.1)**
  - **[3.2. Связь с интеллектом](#3.2)**
  - **[3.3. Если связь сильна](#3.3)**
- **[4. Возможные последствия](#4)**
  - **[4.1. Лучшие сценарии](#4.1)**
  - **[4.2. Худшие сценарии](#4.2)**
  - **[4.2.1. Конфликтующие ценности](#4.2.1)**
  - **[4.2.2. Отклонения в оптимизации среды](#4.2.2)**
  - **[4.2.3. Внутренние проблемы](#4.2.3)**
  - **[4.3. Неоднозначность](#4.3)**
- **[5. Как быть](#5)**
  - **[5.1. Что можно сделать](#5.1)**
  - **[5.2. Чего делать не стоит](#5.2)**

<h2 id="1">1. О чем идет речь</h2>

[Общий искусственный интеллект](https://arbital-ru.github.io/p/agi/) (ОИИ, который еще часто называют "сильным" ИИ) — это гипотетический вид искусственного интеллекта, способный понимать и обучаться решению любых интеллектуальных задач, которые способны решать люди. Этот вид ИИ противопоставляется тем, которые запрограммированы на решение наперед определенного узкого класса задач.

Некоторые считают, что создание все более и более развитых форм ИИ может привести к неконтролируемой [технологической сингулярности](https://ru.wikipedia.org/wiki/Технологическая_сингулярность), которая неузнаваемо изменит наш мир. Так это или нет, но подобно любым другим технологиям, ИИ уже меняет наш мир, и это влияние может значительно вырасти в будущем.

Что почитать:
- Ник Бостром, [Искусственный интеллект. Этапы, Угрозы, Стратегии](https://ru.wikipedia.org/wiki/%D0%98%D1%81%D0%BA%D1%83%D1%81%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9_%D0%B8%D0%BD%D1%82%D0%B5%D0%BB%D0%BB%D0%B5%D0%BA%D1%82._%D0%AD%D1%82%D0%B0%D0%BF%D1%8B,_%D0%A3%D0%B3%D1%80%D0%BE%D0%B7%D1%8B,_%D0%A1%D1%82%D1%80%D0%B0%D1%82%D0%B5%D0%B3%D0%B8%D0%B8) (2014)
- Arbital, [General intelligence](https://arbital.com/p/general_intelligence/)
- Arbital, [Advanced agent properties](https://arbital.com/p/advanced_agent/)

<h2 id="2">2. Когда ждать</h2>

В нашем мире есть множество проблем, с которыми люди справляются с трудом или не могут справиться вовсе — из-за ограничений в вычислительных ресурсах, знаниях и времени, а также из-за конкуренции — поэтому у разных групп людей есть сильная мотивация для создания все более совершенных интеллектуальных систем для оптимизации тех или иных сфер. ОИИ — это естественный итог роста такой оптимизации.

Так что думаю, что с большой вероятностью ОИИ будет создан, рано или поздно, если только для этого нет каких-то принципиальных технических ограничений, непреодолимых людьми вовсе (я думаю, что таких нет), или если прежде человечество не остановит какой-нибудь катаклизм, который приведет к вымиранию.

Но гораздо менее понятно, как скоро появится ОИИ, и каковы будут темпы роста его возможностей — т. е. насколько контролируемым будет процесс его дальнейшего развития.

Что почитать:
- AI Impacts, [AI Timeline Surveys](https://aiimpacts.org/category/ai-timelines/predictions-of-human-level-ai-timelines/ai-timeline-surveys/)
- LessWrong, [AI Timelines](https://www.lesswrong.com/tag/ai-timelines)
- Metaculus, [When will the first general AI system be devised, tested, and publicly announced?](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/)
- Metaculus, [When will the first weakly general AI system be devised, tested, and publicly announced?](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/)
- Brian Tomasik, Artificial Intelligence and Its Implications for Future Suffering. [A case for epistemic modesty on AI timelines](https://longtermrisk.org/artificial-intelligence-and-its-implications-for-future-suffering#A_case_for_epistemic_modesty_on_AI_timelines) (2014-2019)
- LessWrong, [AI Takeoff](https://www.lesswrong.com/tag/ai-takeoff)
- Магнус Виндинг, [Список источников против жесткого взлета ИИ](https://reducingsuffering.github.io/magnus-vinding-a-contra-ai-foom-reading-list.html) (2017-2023)

<h2 id="3">3. Роль ценностей</h2>

<h3 id="3.1">3.1. Кто у руля</h3>

Если по крайней мере две причины, по которым ценности разработчиков ИИ (как и всех прочих влияющих на ход разработки людей — например, политиков-регуляторов и спонсоров) важны для того, каким этот ИИ в итоге окажется.

Во-первых, ценности влияют на то, какие возможные риски людей будут больше волновать, а какие меньше, — что повлияет на ход разработки.

Во-вторых, сами цели ИИ в той или иной степени будут определяться разработчиками. В частности, они могут напрямую попытаться передать ИИ некоторый набор ценностей.

<h3 id="3.2">3.2. Связь с интеллектом</h3>

Широко распространено заблуждение о том, что с какого-то (не особенно далекого) порога интеллектуально развитые существа автоматически за счет своего интеллекта будут приобретать некие "правильные" [терминальные](https://arbital-ru.github.io/p/terminal_vs_instrumental/) ценности и цели. По видимому, это связано с [ошибкой проецирования ума](https://lesswrong.ru/w/%D0%9E%D1%88%D0%B8%D0%B1%D0%BA%D0%B0_%D0%BF%D1%80%D0%BE%D0%B5%D1%86%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F_%D1%83%D0%BC%D0%B0) и [некритичной приверженностью](https://dzen.ru/a/ZY4A7KwDyyzcoIQ6) сильным формам морального реализма: верой в существование объективных моральных истин (о том, какие поступки правильны, а какие нет), знание которых мотивировало бы любой развитый ум вести себя морально (поступать правильно).

В действительности сейчас нет никаких серьезных оснований полагать, что принципиально невозможны сверхинтеллектуальные (т. е. превосходящие интеллектом любого человека) существа, преследующие странные и "скучные" цели вроде [превращения](https://arbital-ru.github.io/p/paperclip_maximizer/) как можно большего количества материи во вселенной в канцелярские скрепки, или получающие удовольствие от чужих страданий. Конечно, некоторые сочетания целей и интеллекта невозможны или крайне неустойчивы, но без них [пространство возможных умов](https://arbital-ru.github.io/p/mind_design_space_wide/) 
едва ли заметно обеднеет.

Что почитать:
- Ник Бостром, Искусственный интеллект. [Связь между интеллектом и мотивацией, Тезис ортогональности](https://lenta.ru/articles/2016/03/06/superintellect/) (2014)
- Arbital, [Orthogonality Thesis](https://arbital.com/p/orthogonality/)
- LessWrong, [Orthogonality Thesis](https://www.lesswrong.com/tag/orthogonality-thesis)
- Элиезер Юдковский, [Нет универсально убедительных аргументов](https://reducingsuffering.github.io/eliezer-yudkowsky-no-universally-compelling-arguments.html) (2008)
- Википедия, [Принцип Юма](https://ru.wikipedia.org/wiki/%D0%9F%D1%80%D0%B8%D0%BD%D1%86%D0%B8%D0%BF_%D0%AE%D0%BC%D0%B0)
- Леонид Максимов, [«Гильотина Юма»: Pro et contra](https://cyberleninka.ru/article/n/gilotina-yuma-pro-et-contra/viewer)
- Андрей Васильев, [Метаэтика: обзор проблематики](https://cyberleninka.ru/article/n/metaetika-obzor-problematiki/viewer)
- Lukas Gloor, [Moral Anti-Realism](https://forum.effectivealtruism.org/s/R8vKwpMtFQ9kDvkJQ) (2018-2022)
- Даниэль Ламан, [Почему не бывает истинных ценностей: о попытках обоснования морали с помощью философии, науки и чего-либо ещё](https://dzen.ru/a/ZY4A7KwDyyzcoIQ6) (2023)

<h3 id="3.3">3.3. Если связь сильна</h3>

Однако, если бы связь между высоким интеллектом и ценностями все же была, что бы это значило? Рассмотрим здесь вариант, что начиная с какого-то гипотетического порога развития интеллекта — назовем его порогом X — цели обладателя этого интеллекта по какой-то неведомой причине либо а) быстро сходятся к некоторой конкретной [функции полезности](https://arbital-ru.github.io/p/utility_function/) (или [системе предпочтений](https://arbital-ru.github.io/p/preference_framework/)) S, либо б) сочетание этого интеллекта с отличающимися от S целями оказывается попросту невозможным на этапе проектирования.

Было бы рано расслабляться, ведь остаются следующие возможности:

1. Порог X может оказаться труднодостижимым и иррелевантным в контексте ближайших столетий. Например, может оказаться, что его достижение требует вычислительных ресурсов, получаемых из целой солнечной системы.
2. ИИ может не дотягивать до порога X, но все же очень сильно влиять на мир — например, если его производство будет поставлено на поток и в мире будет достаточно много его экземпляров, или ему доверят большую власть, или им будут пользоваться отдельные влиятельные группы людей.
3. Если есть возможность узнать о существовании такого порога X и примерно определить его, создатели ИИ могут препятствовать развитию интеллекта до этого порога, чтобы ИИ не вышел из под их контроля.
4. Цель S, которую обретает ИИ по достижению порога X, может оказаться далекой от уменьшения страданий во вселенной. Например, он мог бы максимизировать число скрепок (или иной [странной ерунды](https://arbital.com/p/paperclip/)) во вселенной, или число доказанных математических теорем, или по иным причинам быть равнодушным к благополучию других.
5. Даже если достижение порога X приводит к благой цели вроде уменьшения страданий во вселенной, такой ИИ может быть косвенно подчинен неблагим целям. Например, он может [содержаться](https://22century.ru/popular-science-publications/leakproofing-the-singularity) в виртуальной реальности, не имея представления о внешнем мире, в то время как создатели, вводящие его в заблуждение, извлекают какую-то выгоду из его работы и используют в не столь благих целях.

<h2 id="4">4. Возможные последствия</h2>
 
<h3 id="4.1">4.1. Хорошие сценарии?</h3>

Может ли ИИ сделать мир лучше? На ум приходит множество вариантов направлений оптимизации человеческого общества, биосферы планеты и более широких областей вселенной, где пригодился бы контролируемый ИИ:

- Развитие медицины и генной инженерии, а также попытки в целом победить болезни, [старость и смерть](https://ru.wikipedia.org/wiki/%D0%98%D0%BC%D0%BC%D0%BE%D1%80%D1%82%D0%B0%D0%BB%D0%B8%D0%B7%D0%BC), или [биологические страдания как таковые](https://reducingsuffering.github.io/89.html).
- Развитие взаимопонимания, сострадания и сотрудничества, предотвращение конфликтов, приближение всеобщего мира.
- Исследование страданий животных (особенно в дикой природе, где их намного больше) и поиск социально и технологически реалистичных подходов к их уменьшению.
- Развитие [астробиологии](https://ubq124.wordpress.com/2016/12/23/david-catling-astrobiology-russian-translate/) и космических технологий с целью оценки распространенности страданий в космосе и поиска способов их уменьшить.

Однако, все эти сценарии подразумевают, что ИИ
 
<h3 id="4.2">4.2. Плохие сценарии</h3>

Я рассмотрю три категории связанных с ИИ рисков значительных страданий по их происхождению. Первая категория связана с успешной реализацией через ИИ каких-либо человеческих ценностей, но за счет увеличения страданий. Вторая связана с ошибками в проектировании ИИ, приводящими к нежелательным вариантам оптимизации окружающей его среды. Третья — с тем, что сам ИИ может испытывать страдание или содержать страдающих существ внутри.

Что почитать:
- Tobias Baumann, [S-risks: An introduction](https://centerforreducingsuffering.org/research/intro/)
- Kaj Sotala & Lukas Gloor, [Superintelligence as a Cause or Cure for Risks of Astronomical Suffering](https://www.informatica.si/index.php/informatica/article/viewFile/1877/1098)
- Дэн Хендрикс, Мантас Мазейка и Томас Вудсайд, [Обзор катастрофических рисков ИИ](https://lesswrong.ru/%D0%9E%D0%B1%D0%B7%D0%BE%D1%80_%D0%BA%D0%B0%D1%82%D0%B0%D1%81%D1%82%D1%80%D0%BE%D1%84%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D1%85_%D1%80%D0%B8%D1%81%D0%BA%D0%BE%D0%B2_%D0%98%D0%98)
 (2023)
 
#### **4.2.1. Конфликтующие ценности**

Есть много пугающих сценариев развития будущего, которые, тем не менее, будут считаться сравнительно хорошими с точки зрения тех или иных этических систем, и если соответствующие этические взгляды унаследует могущественный ИИ — мы можем с какой-то вероятностью ожидать реализации этих сценариев.

Простейший вариант — реализация систем ценностей, в которых никакое страдание не является отрицательно ценным (и, может быть, вообще нет ничего отрицательно ценного).

Например, [панбиотическая этика](https://philarchive.org/rec/MAULEA) объявляет положительно ценной жизнь, и в чистом виде может не предполагать ничего отрицательно ценного. Таким образом, с точки зрения панбиотической этики идея [засевать](https://kkirdan.github.io/blog/c1.html) все доступные планеты вселенной жизнью звучит хорошо, но с точки зрения [негативного утилитаризма](https://reducingsuffering.github.io/what-is-negative-utilitarianism.html) это звучит как катастрофа.

Значительно более мягким примером могут служить системы ценностей, приоритизирующие невмешательство в жизнь других видов. Они могут выступать против заселения необитаемых планет кем-либо кроме представителей нашего вида — что хорошо, но в то же время они будут выступать против гедонистической оптимизации жизни других видов. Кроме того, возможно, что они будут довольно равнодушными к судьбе нашего собственного вида.

Менее очевиден конфликт в случае системы ценностей, диктующей максимизацию усредненной длительности жизни достаточно разумных существ (например, не менее разумных, чем люди). Может быть, в реальности такое увеличение продолжительности жизни будет хорошо коррелировать с уменьшением страданий. Но это необязательно так.

Возможно, ведомые такими ценностями цивилизации не будут заинтересованы в гедонистической оптимизации окружающей их биосферы (и тем более — биосферы других планет). Более того, могут существовать пути увеличения средней длительности жизни, требующие интенсивных и/или длительных страданий. Что, если хорошим способом увеличить среднюю продолжительность жизни окажется создание и обслуживание специальных тюрем, в которых содержатся страдающие пост-люди, лишенные возможности умереть?

Рассмотрим также системы ценностей, которые признают отрицательность тех или иных классов страданий, но допускают моральную компенсируемость страданий какими-нибудь положительными явлениями. Например, бентамовский классический утилитаризм подразумевает положительную ценность любого удовольствия и отрицательную ценность любого страдания.

Один из классических примеров — это так называемое «отвратительное следствие» (англ. [repugnant conclusion](https://plato.stanford.edu/entries/repugnant-conclusion/)), согласно которому правильным будет неограниченное увеличение популяции живых существ с одновременным снижением их благополучия, пока оно остается выше нуля и пока суммарное благополучие растет.

Другие примеры: садизм и утилитарный монстр.
<b class="draft">Не дописано.</b>

Что почитать:
- Брайан Томасик, [Умножит ли колонизация космоса страдания диких животных?](https://reducingsuffering.github.io/brian-tomasik-will-space-colonization-multiply-wild-animal-suffering.html) (2014-2018)
- Брайан Томасик, [Омелас и колонизация космоса](https://reducingsuffering.github.io/brian-tomasik-omelas-and-space-colonization.html) (2013-2017)

#### **4.2.2. Отклонения в оптимизации среды**

<b class="draft">Не дописано.</b>

Что почитать:
- Элиезер Юдковский, [Скрытая сложность желаний](https://lesswrong.ru/w/%D0%A1%D0%BA%D1%80%D1%8B%D1%82%D0%B0%D1%8F_%D1%81%D0%BB%D0%BE%D0%B6%D0%BD%D0%BE%D1%81%D1%82%D1%8C_%D0%B6%D0%B5%D0%BB%D0%B0%D0%BD%D0%B8%D0%B9) (2007)
- Элиезер Юдковский, [Задача соответствия ракет и цели](https://lesswrong.ru/w/%D0%97%D0%B0%D0%B4%D0%B0%D1%87%D0%B0_%D1%81%D0%BE%D0%BE%D1%82%D0%B2%D0%B5%D1%82%D1%81%D1%82%D0%B2%D0%B8%D1%8F_%D1%80%D0%B0%D0%BA%D0%B5%D1%82_%D0%B8_%D1%86%D0%B5%D0%BB%D0%B8) (2018)
- Brian Tomasik, Artificial Intelligence and Its Implications for Future Suffering. [Would a human-inspired AI or rogue AI cause more suffering?](https://longtermrisk.org/artificial-intelligence-and-its-implications-for-future-suffering#Would_a_human-inspired_AI_or_rogue_AI_cause_more_suffering) (2014-2019)
- Brian Tomasik, [Astronomical suffering from slightly misaligned artificial intelligence](https://reducing-suffering.org/near-miss/) (2018-2019)
- Arbital, [Separation from hyperexistential risk](https://arbital.com/p/hyperexistential_separation/)

#### **4.2.3. Внутренние проблемы**

<b class="draft">Не дописано.</b>

Что почитать:
- Soenke Ziesche, Roman Yampolskiy, [Towards AI Welfare Science and Policies](https://www.mdpi.com/2504-2289/3/1/2) (2018)
- Томас Метцингер, [Наука о мозге и миф о своем Я. Тоннель Эго](https://batrachos.com/sites/default/files/pictures/Books/Mettsinger_2016_Nauka o mozge i mif o svoem ya.pdf). Глава 7. Искусственные эго-машины (2009)
- Томас Метцингер, [Искусственное страдание: Аргумент в пользу глобального моратория на синтетическую феноменологию](https://vk.com/wall-194967191_1276) (2021)
- Arbital, [Ментальное преступление: Введение](https://arbital-ru.github.io/p/mindcrime_introduction/)
- Arbital, [Ментальное преступление](https://arbital-ru.github.io/p/mindcrime/)
- Brian Tomasik, [What Are Suffering Subroutines?](https://reducing-suffering.org/what-are-suffering-subroutines/) (2017-2019)
- Brian Tomasik, [A Dialogue on Suffering Subroutines](https://longtermrisk.org/a-dialogue-on-suffering-subroutines/) (2013-2017)

<h3 id="4.3">4.3. Неоднозначность</h3>

Итак, успешная реализация ряда человеческих систем ценностей могла бы привести к большим страданиям. С другой стороны, и неудача такой реализации из-за неверного согласования тоже может привести к большим страданиям. И даже если история человечества окончится созданием крайне далекого от согласованности с кем-либо ОИИ (например, максимизатора скрепок), он тоже может создать массу страданий по инструментальным причинам.

Наконец, если представить, что человечество вымрет, не создав никакого ОИИ, этот сценарий тоже не будет свободным от страданий. Во-первых, страдают не только (и возможно, [не столько](https://reducingsuffering.github.io/424.html)) люди, но и [другие животные](https://ru.wikipedia.org/wiki/%D0%A1%D1%82%D1%80%D0%B0%D0%B4%D0%B0%D0%BD%D0%B8%D1%8F_%D0%B4%D0%B8%D0%BA%D0%B8%D1%85_%D0%B6%D0%B8%D0%B2%D0%BE%D1%82%D0%BD%D1%8B%D1%85) на этой планете. Более того, в доступной для захвата и оптимизации области пространства-времени могут быть другие населенные дикими страдающими животными планеты в неизвестном количестве. Во-вторых, близлежащий космос может быть в будущем [захвачен другими разумными существами](https://reducingsuffering.github.io/483.html) и неясно, хуже был бы этот сценарий в сравнении с захватом людьми, или лучше.

Короче говоря, довольно сложно судить о том, как различные исходы упорядочены между собой с точки зрения количества ожидаемых в них страданий. Сложно судить и о том, каковы вероятности наступления этих исходов. И в конце концов нам нужно понять, что мы вообще можем сделать и какова величина нашего потенциального влияния на те или иные исходы.

Что почитать:
- Brian Tomasik, 
[Risks of Astronomical Future Suffering](https://longtermrisk.org/risks-of-astronomical-future-suffering/) (2011-2019)
- Magnus Vinding, [Can we confidently dismiss the existence of near aliens? Probabilities and implications](https://magnusvinding.com/2023/07/24/can-we-confidently-dismiss-the-existence-of-near-aliens-probabilities-and-implications/) (2023)

<h2 id="5">5. Как быть</h2>

<h2 id="5.1">5.1. Что можно сделать</h2>

Самое очевидное, что можно сделать — это вкатываться в тему безопасности ИИ и заниматься рисками, которые нас интересуют, или поддерживать работу в этой сфере других людей с близкими нам ценностями.

Если вы считаете, что согласованный ИИ в среднем лучше несогласованного, можно заняться проблемой согласования (например, поддерживая работу MIRI и подобных центров). Если же наоборот, то лучше заняться направлениями безопасности ИИ, фокусирующимися не на согласованности как таковой, а на избегании наихудших сценариев вроде s-рисков.

С другой стороны, можно пытаться влиять на ценности тех, кто может заниматься разработкой ИИ или иным образом существенно влиять на нее. Это могут быть ценности человечества в целом, или же населения более технологически продвинутых стран и/или представителей каких-то отдельных социальных групп (технические эксперты, политики).

Другой вариант — вместо влияния на фундаментальные ценности повышать у этих людей видимость и степень важности рисков, которые нас больше всего беспокоят, и к которым они тоже неравнодушны. Таким образом, принимая решения, они уделят больше внимания соответствующим направлениям безопасности.

Что почитать:
- Lukas Gloor, [Altruists Should Prioritize Artificial Intelligence](https://longtermrisk.org/altruists-should-prioritize-artificial-intelligence/) (2016-2019)
- Magnus Vinding, [Why Altruists Should Perhaps Not Prioritize Artificial Intelligence: A Lengthy Critique](https://magnusvinding.com/2018/09/18/why-altruists-should-perhaps-not-prioritize-artificial-intelligence-a-lengthy-critique/) (2018)
- Brian Tomasik, [Artificial Intelligence and Its Implications for Future Suffering](https://longtermrisk.org/artificial-intelligence-and-its-implications-for-future-suffering) (2014-2019)
- Tobias Baumann, [An introduction to worst-case AI safety](https://s-risks.org/an-introduction-to-worst-case-ai-safety/) (2018)

<h3 id="5.2">5.1. Чего делать не стоит</h3>

Вряд ли на пользу пойдет активная конфронтация с другими системами ценностей, представители которых хотят получить свой кусок общего пирога.

В частности, пытаться единолично создать ОИИ, преследуя цель уменьшения страданий (или такую, которую вы считаете ее аппроксимацией) — вряд ли хорошая идея, и тому есть куча причин.

<b class="draft">Не дописано.</b>

Что почитать:
- Брайан Томасик, Риски астрономических будущих страданий. [Почему мы должны оставаться кооперативными](https://reducingsuffering.github.io/brian-tomasik-1.html) (2011-2019)
- Teo Ajantaival, [Peacefulness, nonviolence, and experientialist minimalism](https://forum.effectivealtruism.org/s/MBadsrYLmzLNmYjaj/p/JnHeeTGAohMFxNbGK) (2022)
- [Negative Utilitarianism FAQ](https://www.utilitarianism.com/nu/nufaq.html)
