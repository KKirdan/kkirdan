---
layout: post
title: Искусственный интеллект и страдание
show_title: true
updated: 2024-06-18
unixtime:
  - 1622557140
  - 1678625715
vk: https://vk.com/wall-178968945_414
medium: https://medium.com/@k.kirdan/%D1%81%D0%B8%D0%BB%D1%8C%D0%BD%D1%8B%D0%B9-%D0%B8%D1%81%D0%BA%D1%83%D1%81%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9-%D0%B8%D0%BD%D1%82%D0%B5%D0%BB%D0%BB%D0%B5%D0%BA%D1%82-%D0%B8-%D1%83%D0%BC%D0%B5%D0%BD%D1%8C%D1%88%D0%B5%D0%BD%D0%B8%D0%B5-%D1%81%D1%82%D1%80%D0%B0%D0%B4%D0%B0%D0%BD%D0%B8%D0%B9-52cf7f1b10fa
tags:
  - искусственный_интеллект
  - глобальные_риски
  - счастье_и_несчастье
meta_tags: черновик
importance: 9
---
Как людям, желающим (главным образом) [уменьшения страданий во вселенной](https://reducingsuffering.github.io/what-is-suffering-reduction.html), относиться к разработке все более и более мощных форм искусственного интеллекта? ИИ может стать причиной многих страданий (в частности, сам их испытывать), или же напротив — может сыграть какую-то роль в их уменьшении. Особенное беспокойство вызывают идеи общего искусственного интеллекта (ОИИ) и/или сверхинтеллекта.

Я изложу здесь некоторые из своих соображений по этой теме и предоставлю ссылки для более глубокого изучения.

<b class="draft">Статья сейчас находится в процессе переписывания.</b>

## Содержание

- **[1. О чем идет речь](#1)**
- **[2. Когда ждать](#2)**
- **[3. Роль ценностей](#3)**
  - **[3.1. Кто у руля](#3.1)**
  - **[3.2. Связь с интеллектом](#3.2)**
  - **[3.3. Если связь сильна](#3.3)**
- **[4. Возможные последствия](#4)**
  - **[4.1. Худшие сценарии](#4.1)**
  - **[4.1.1. Конфликтующие ценности](#4.1.1)**
  - **[4.1.2. Неудачное согласование](#4.1.2)**
  - **[4.1.3. Внутренние страдания](#4.1.3)**
  - **[4.2. Лучшие сценарии](#4.2)**
  - **[4.3. Неоднозначность](#4.3)**
- **[5. Как быть](#5)**
  - **[5.1. Что можно сделать](#5.1)**
  - **[5.2. Чего делать не стоит](#5.2)**

<h2 id="1">1. О чем идет речь</h2>

[Общий искусственный интеллект](https://arbital-ru.github.io/p/agi/) (ОИИ, который еще часто называют "сильным" ИИ) — это гипотетический вид искусственного интеллекта, способный понимать и обучаться решению любых интеллектуальных задач, которые способны решать люди. Этот вид ИИ противопоставляется тем, которые запрограммированы на решение наперед определенного узкого класса задач.

Некоторые считают, что создание все более и более развитых форм ИИ может привести к неконтролируемой [технологической сингулярности](https://ru.wikipedia.org/wiki/Технологическая_сингулярность), которая неузнаваемо изменит наш мир. Так это или нет, но подобно любым другим технологиям, ИИ уже меняет наш мир, и это влияние может значительно вырасти в будущем.

Что почитать:
- Ник Бостром, [Искусственный интеллект. Этапы, Угрозы, Стратегии](https://ru.wikipedia.org/wiki/%D0%98%D1%81%D0%BA%D1%83%D1%81%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9_%D0%B8%D0%BD%D1%82%D0%B5%D0%BB%D0%BB%D0%B5%D0%BA%D1%82._%D0%AD%D1%82%D0%B0%D0%BF%D1%8B,_%D0%A3%D0%B3%D1%80%D0%BE%D0%B7%D1%8B,_%D0%A1%D1%82%D1%80%D0%B0%D1%82%D0%B5%D0%B3%D0%B8%D0%B8) (2014)
- Arbital, [General intelligence](https://arbital.com/p/general_intelligence/)
- Arbital, [Advanced agent properties](https://arbital.com/p/advanced_agent/)

<h2 id="2">2. Когда ждать</h2>

В нашем мире есть множество проблем, с которыми люди справляются с трудом или не могут справиться вовсе — из-за ограничений в вычислительных ресурсах, знаниях и времени, а также из-за конкуренции — поэтому у разных групп людей есть сильная мотивация для создания все более совершенных интеллектуальных систем для оптимизации тех или иных сфер. ОИИ — это естественный итог роста такой оптимизации.

Так что думаю, что с большой вероятностью ОИИ будет создан, рано или поздно, если только для этого нет каких-то принципиальных технических ограничений, непреодолимых людьми вовсе (я думаю, что таких нет), или если прежде человечество не остановит какой-нибудь катаклизм, который приведет к вымиранию.

Но гораздо менее понятно, как скоро появится ОИИ, и каковы будут темпы роста его возможностей — т. е. насколько контролируемым будет процесс его дальнейшего развития.

Что почитать:
- AI Impacts, [AI Timeline Surveys](https://aiimpacts.org/category/ai-timelines/predictions-of-human-level-ai-timelines/ai-timeline-surveys/)
- LessWrong, [AI Timelines](https://www.lesswrong.com/tag/ai-timelines)
- Metaculus, [When will the first general AI system be devised, tested, and publicly announced?](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/)
- Metaculus, [When will the first weakly general AI system be devised, tested, and publicly announced?](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/)
- Brian Tomasik, Artificial Intelligence and Its Implications for Future Suffering. [A case for epistemic modesty on AI timelines](https://longtermrisk.org/artificial-intelligence-and-its-implications-for-future-suffering#A_case_for_epistemic_modesty_on_AI_timelines) (2014-2019)
- LessWrong, [AI Takeoff](https://www.lesswrong.com/tag/ai-takeoff)
- Магнус Виндинг, [Список источников против жесткого взлета ИИ](https://reducingsuffering.github.io/magnus-vinding-a-contra-ai-foom-reading-list.html) (2017-2023)

<h2 id="3">3. Роль ценностей</h2>

<h3 id="3.1">3.1. Кто у руля</h3>

Если по крайней мере две причины, по которым ценности разработчиков ИИ (как и всех прочих влияющих на ход разработки людей — например, политиков-регуляторов и спонсоров) важны для того, каким этот ИИ в итоге окажется.

Во-первых, ценности влияют на то, какие возможные риски людей будут больше волновать, а какие меньше, — что повлияет на ход разработки.

Во-вторых, сами цели ИИ в той или иной степени будут определяться разработчиками. В частности, они могут напрямую попытаться передать ИИ некоторый набор ценностей.

<h3 id="3.2">3.2. Связь с интеллектом</h3>

Широко распространено заблуждение о том, что с какого-то (не особенно далекого) порога интеллектуально развитые существа автоматически за счет своего интеллекта будут приобретать некие "правильные" [терминальные](https://arbital-ru.github.io/p/terminal_vs_instrumental/) ценности и цели. По видимому, это связано с [ошибкой проецирования ума](https://lesswrong.ru/w/%D0%9E%D1%88%D0%B8%D0%B1%D0%BA%D0%B0_%D0%BF%D1%80%D0%BE%D0%B5%D1%86%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F_%D1%83%D0%BC%D0%B0) и [некритичной приверженностью](https://dzen.ru/a/ZY4A7KwDyyzcoIQ6) сильным формам морального реализма: верой в существование объективных моральных истин (о том, какие поступки правильны, а какие нет), знание которых мотивировало бы любой развитый ум вести себя морально (поступать правильно).

В действительности сейчас нет никаких серьезных оснований полагать, что принципиально невозможны сверхинтеллектуальные (т. е. превосходящие интеллектом любого человека) существа, преследующие странные и "скучные" цели вроде [превращения](https://arbital-ru.github.io/p/paperclip_maximizer/) как можно большего количества материи во вселенной в канцелярские скрепки, или получающие удовольствие от чужих страданий.

Конечно, некоторые сочетания целей и интеллекта невозможны или крайне неустойчивы, но без них [пространство возможных умов](https://arbital-ru.github.io/p/mind_design_space_wide/) 
едва ли заметно обеднеет.

Что почитать:
- Ник Бостром, Искусственный интеллект. [Связь между интеллектом и мотивацией, Тезис ортогональности](https://lenta.ru/articles/2016/03/06/superintellect/) (2014)
- Arbital, [Orthogonality Thesis](https://arbital.com/p/orthogonality/)
- LessWrong, [Orthogonality Thesis](https://www.lesswrong.com/tag/orthogonality-thesis)
- Элиезер Юдковский, [Нет универсально убедительных аргументов](https://reducingsuffering.github.io/eliezer-yudkowsky-no-universally-compelling-arguments.html) (2008)
- Википедия, [Принцип Юма](https://ru.wikipedia.org/wiki/%D0%9F%D1%80%D0%B8%D0%BD%D1%86%D0%B8%D0%BF_%D0%AE%D0%BC%D0%B0)
- Леонид Максимов, [«Гильотина Юма»: Pro et contra](https://cyberleninka.ru/article/n/gilotina-yuma-pro-et-contra/viewer)
- Андрей Васильев, [Метаэтика: обзор проблематики](https://cyberleninka.ru/article/n/metaetika-obzor-problematiki/viewer)
- Lukas Gloor, [Moral Anti-Realism](https://forum.effectivealtruism.org/s/R8vKwpMtFQ9kDvkJQ) (2018-2022)
- Даниэль Ламан, [Почему не бывает истинных ценностей: о попытках обоснования морали с помощью философии, науки и чего-либо ещё](https://dzen.ru/a/ZY4A7KwDyyzcoIQ6) (2023)

<h3 id="3.3">3.3. Если связь сильна</h3>

Однако, если бы связь между высоким интеллектом и ценностями все же была, что бы это значило? Рассмотрим здесь вариант, что начиная с какого-то гипотетического порога развития интеллекта — назовем его порогом X — цели обладателя этого интеллекта по какой-то неведомой причине либо а) быстро сходятся к некоторой конкретной [функции полезности](https://arbital-ru.github.io/p/utility_function/) (или [системе предпочтений](https://arbital-ru.github.io/p/preference_framework/)) S, либо б) сочетание этого интеллекта с отличающимися от S целями оказывается попросту невозможным на этапе проектирования.

Было бы рано расслабляться, ведь остаются следующие возможности:

1. Порог X может оказаться труднодостижимым и иррелевантным в контексте ближайших столетий. Например, может оказаться, что его достижение требует вычислительных ресурсов, получаемых из целой солнечной системы.
2. ИИ может не дотягивать до порога X, но все же очень сильно влиять на мир — например, если его производство будет поставлено на поток и в мире будет достаточно много его экземпляров, или ему доверят большую власть, или им будут пользоваться отдельные влиятельные группы людей.
3. Если есть возможность узнать о существовании такого порога X и примерно определить его, создатели ИИ могут препятствовать развитию интеллекта до этого порога, чтобы ИИ не вышел из под их контроля.
4. Цель S, которую обретает ИИ по достижению порога X, может оказаться далекой от уменьшения страданий во вселенной. Например, он мог бы становиться полностью эгоистичным и равнодушным к благополучию других.
5. Даже если достижение порога X приводит к благой цели вроде уменьшения страданий во вселенной, такой ИИ может быть косвенно подчинен неблагим целям. Например, он может содержаться в виртуальной реальности, не имея представления о внешнем мире, в то время как создатели, вводящие его в заблуждение, извлекают какую-то выгоду из его работы и используют в не столь благих целях.

<h2 id="4">4. Возможные последствия</h2>
 
<h3 id="4.1">4.1. Худшие сценарии</h3>

Сценарии, в которых порождаются объемы страданий, многократно превышающие все, что уже было перенесено населением нашей планеты, называются сценариями [s-рисков](https://centerforreducingsuffering.org/research/intro/) (s — от англ. "suffering" — "страдание") или рисков астрономических страданий.

Я рассмотрю 3 категории потенциальных s-рисков по их происхождению. Первая категория связана с успешной реализацией людьми каких-либо ценностей, но за счет увеличения страданий. Вторая связана с ошибками в проектировании ИИ, приводящими к нежелательным вариантам оптимизации окружающей его среды. Третья — с тем, что сам ИИ может испытывать страдание или содержать страдающих существ внутри.

Что почитать:
- Tobias Baumann, [S-risks: An introduction](https://centerforreducingsuffering.org/research/intro/)
- Kaj Sotala & Lukas Gloor, [Superintelligence as a Cause or Cure for Risks of Astronomical Suffering](https://www.informatica.si/index.php/informatica/article/viewFile/1877/1098)
- Дэн Хендрикс, Мантас Мазейка и Томас Вудсайд, [Обзор катастрофических рисков ИИ](https://lesswrong.ru/%D0%9E%D0%B1%D0%B7%D0%BE%D1%80_%D0%BA%D0%B0%D1%82%D0%B0%D1%81%D1%82%D1%80%D0%BE%D1%84%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D1%85_%D1%80%D0%B8%D1%81%D0%BA%D0%BE%D0%B2_%D0%98%D0%98)
 (2023)
 
#### **4.1.1. Конфликтующие ценности**

Есть много пугающих сценариев развития будущего, которые, тем не менее, будут считаться сравнительно хорошими с точки зрения тех или иных этических систем, и если соответствующие этические взгляды унаследует могущественный ИИ — мы можем с какой-то вероятностью ожидать реализации этих сценариев.

Простейший вариант — реализация систем ценностей, в которых никакое страдание не является отрицательно ценным (и, может быть, вообще нет ничего отрицательно ценного).

Например, [панбиотическая этика](https://philarchive.org/rec/MAULEA) объявляет положительно ценной жизнь, и в чистом виде может не предполагать ничего отрицательно ценного. Таким образом, с точки зрения панбиотической этики идея [засевать](https://kkirdan.github.io/blog/c1.html) все доступные планеты вселенной жизнью звучит хорошо, но с точки зрения [негативного утилитаризма](https://reducingsuffering.github.io/what-is-negative-utilitarianism.html) это звучит как катастрофа.

Значительно более мягким примером могут служить системы ценностей, приоритизирующие невмешательство в жизнь других видов. Они могут выступать против заселения необитаемых планет кем-либо кроме представителей нашего вида — что хорошо, но в то же время они будут выступать против гедонистической оптимизации жизни других видов. Кроме того, возможно, что они будут довольно равнодушными к судьбе нашего собственного вида.

Менее очевиден конфликт в случае системы ценностей, диктующей максимизацию усредненной длительности жизни достаточно разумных существ (например, не менее разумных, чем люди). Может быть, в реальности такое увеличение продолжительности жизни будет хорошо коррелировать с уменьшением страданий. Но это необязательно так.

Возможно, ведомые такими ценностями цивилизации не будут заинтересованы в гедонистической оптимизации окружающей их биосферы (и тем более — биосферы других планет). Более того, могут существовать пути увеличения средней длительности жизни, требующие интенсивных и/или длительных страданий. Что, если довольно простым способом увеличить среднюю продолжительность жизни окажется создание и обслуживание специальных тюрем, в которых содержатся страдающие пост-люди, лишенные возможности умереть?

Рассмотрим также системы ценностей, которые признают отрицательность тех или иных классов страданий, но допускают моральную компенсируемость страданий какими-нибудь положительными явлениями. Например, бентамовский классический утилитаризм подразумевает положительную ценность любого удовольствия и отрицательную ценность любого страдания.

Один из классических примеров — это так называемое «отвратительное следствие» (англ. [repugnant conclusion](https://plato.stanford.edu/entries/repugnant-conclusion/)), согласно которому правильным будет неограниченное увеличение популяции живых существ с одновременным снижением их благополучия, пока оно остается выше нуля и пока суммарное благополучие растет.

Другие примеры: садизм и утилитарный монстр.
<b class="draft">Не дописано.</b>

Что почитать:
- Брайан Томасик, [Умножит ли колонизация космоса страдания диких животных?](https://reducingsuffering.github.io/brian-tomasik-will-space-colonization-multiply-wild-animal-suffering.html) (2014-2018)
- Брайан Томасик, [Омелас и колонизация космоса](https://reducingsuffering.github.io/brian-tomasik-omelas-and-space-colonization.html) (2013-2017)

#### **4.1.2. Ошибки согласования**

<b class="draft">Не дописано.</b>

Что почитать:
- Элиезер Юдковский, [Скрытая сложность желаний](https://lesswrong.ru/w/%D0%A1%D0%BA%D1%80%D1%8B%D1%82%D0%B0%D1%8F_%D1%81%D0%BB%D0%BE%D0%B6%D0%BD%D0%BE%D1%81%D1%82%D1%8C_%D0%B6%D0%B5%D0%BB%D0%B0%D0%BD%D0%B8%D0%B9) (2007)
- Элиезер Юдковский, [Задача соответствия ракет и цели](https://lesswrong.ru/w/%D0%97%D0%B0%D0%B4%D0%B0%D1%87%D0%B0_%D1%81%D0%BE%D0%BE%D1%82%D0%B2%D0%B5%D1%82%D1%81%D1%82%D0%B2%D0%B8%D1%8F_%D1%80%D0%B0%D0%BA%D0%B5%D1%82_%D0%B8_%D1%86%D0%B5%D0%BB%D0%B8) (2018)
- Brian Tomasik, [Astronomical suffering from slightly misaligned artificial intelligence](https://reducing-suffering.org/near-miss/) (2018-2019)

#### **4.1.3. Внутренние страдания**

<b class="draft">Не дописано.</b>

Что почитать:
- Soenke Ziesche, Roman Yampolskiy, [Towards AI Welfare Science and Policies](https://www.mdpi.com/2504-2289/3/1/2) (2018)
- Томас Метцингер, [Наука о мозге и миф о своем Я. Тоннель Эго](https://batrachos.com/sites/default/files/pictures/Books/Mettsinger_2016_Nauka o mozge i mif o svoem ya.pdf). Глава 7. Искусственные эго-машины (2009)
- Томас Метцингер, [Искусственное страдание: Аргумент в пользу глобального моратория на синтетическую феноменологию](https://vk.com/wall-194967191_1276) (2021)
- Arbital, [Ментальное преступление: Введение](https://arbital-ru.github.io/p/mindcrime_introduction/)
- Arbital, [Ментальное преступление](https://arbital-ru.github.io/p/mindcrime/)
- Brian Tomasik, [What Are Suffering Subroutines?](https://reducing-suffering.org/what-are-suffering-subroutines/) (2017-2019)
- Brian Tomasik, [A Dialogue on Suffering Subroutines](https://longtermrisk.org/a-dialogue-on-suffering-subroutines/) (2013-2017)

<h3 id="4.2">4.2. Лучшие сценарии</h3>

С другой стороны, может ли ИИ сделать мир лучше? Конечно, есть множество вариантов оптимизации человеческого общества, биосферы планеты и более широких областей вселенной, где пригодился бы контролируемый ИИ:

- Развитие медицины и генной инженерии, попытки победить болезни, старость и смерть.
- Развитие взаимопонимания, сострадания и сотрудничества, предотвращение конфликтов, приближение всеобщего мира.
- Моделирование и создание постлюдей и постчеловеческих сообществ с меньшим уровнем внутренних страданий и большим уровнем сострадания к другим.
- Исследование страданий животных в дикой природе и поиск социально и технологически реалистичных подходов к их уменьшению.
- Развитие астробиологии и космических технологий с целью оценки распространенности страданий в космосе и поиска способов их уменьшить.
- Исследование границ сознания и сентиентности с целью понимания того, какие конфигурации материи могут страдать и насколько сильно.

<h3 id="4.3">4.3. Роль вымирания</h3>

<b class="draft">Не дописано.</b>

<h2 id="5">5. Как быть</h2>

<h2 id="5.1">5.1. Что можно сделать</h2>

Самое очевидное, что можно сделать — это вкатываться в тему безопасности ИИ и заниматься рисками, которые нас интересуют, или поддерживать работу в этой сфере других людей с близкими нам ценностями.

Если вы считаете, что согласованный ИИ в среднем лучше несогласованного, можно заняться проблемой согласования (например, поддерживая работу MIRI и подобных центров). Если же наоборот, то лучше заняться направлениями безопасности ИИ, фокусирующимися не на согласованности как таковой, а на избегании наихудших сценариев вроде s-рисков.

С другой стороны, можно пытаться влиять на ценности тех, кто может заниматься разработкой ИИ или иным образом существенно влиять на нее. Это могут быть ценности человечества в целом, или же населения более технологически продвинутых стран и/или представителей каких-то отдельных социальных групп (технические эксперты, политики).

Другой вариант — вместо влияния на фундаментальные ценности повышать у этих людей видимость и степень важности рисков, которые нас больше всего беспокоят, и к которым они тоже неравнодушны. Таким образом, принимая решения, они уделят больше внимания соответствующим направлениям безопасности.

Что почитать:
- Lukas Gloor, [Altruists Should Prioritize Artificial Intelligence](https://longtermrisk.org/altruists-should-prioritize-artificial-intelligence/)
- Magnus Vinding, [Why Altruists Should Perhaps Not Prioritize Artificial Intelligence: A Lengthy Critique](https://magnusvinding.com/2018/09/18/why-altruists-should-perhaps-not-prioritize-artificial-intelligence-a-lengthy-critique/)
- Brian Tomasik, [Artificial Intelligence and Its Implications for Future Suffering](https://longtermrisk.org/artificial-intelligence-and-its-implications-for-future-suffering)
- Tobias Baumann, [An introduction to worst-case AI safety](https://s-risks.org/an-introduction-to-worst-case-ai-safety/)

<h3 id="5.2">5.1. Чего делать не стоит</h3>

Вряд ли на пользу пойдет активная конфронтация с другими системами ценностей, представители которых хотят получить свой кусок общего пирога.

В частности, пытаться самостоятельно единолично создать ОИИ — вряд ли хорошая идея, и тому есть куча причин.

<b class="draft">Не дописано.</b>
