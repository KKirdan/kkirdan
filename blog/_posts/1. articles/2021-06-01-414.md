---
layout: post
title: Искусственный интеллект и страдание
show_title: true
updated: 2023-03-12
unixtime: [1622557140, 1678625715]
vk: https://vk.com/wall-178968945_414
medium: https://medium.com/@k.kirdan/%D1%81%D0%B8%D0%BB%D1%8C%D0%BD%D1%8B%D0%B9-%D0%B8%D1%81%D0%BA%D1%83%D1%81%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9-%D0%B8%D0%BD%D1%82%D0%B5%D0%BB%D0%BB%D0%B5%D0%BA%D1%82-%D0%B8-%D1%83%D0%BC%D0%B5%D0%BD%D1%8C%D1%88%D0%B5%D0%BD%D0%B8%D0%B5-%D1%81%D1%82%D1%80%D0%B0%D0%B4%D0%B0%D0%BD%D0%B8%D0%B9-52cf7f1b10fa
tags: ["искусственный_интеллект", "ценности/мораль/этика", "глобальные_риски"]
meta_tags: статья
importance: 9
---
Как людям, желающим минимизировать страдание в мире (будь то [негативные утилитаристы](435.html), [антинаталисты](https://vk.com/wall-199052526_158) или кто-то еще) относиться к разработке сильного (и возможно, [сентиентного](466.html)) искусственного интеллекта в будущем?

Он может быть причиной многих страданий, может сам их испытывать, или же напротив — может сыграть какую-то роль в их уменьшении. Разумнее ли пытаться предотвратить его создание, замедлить, или же принять участие в нем? (Хоть я и пишу о нем в единственном числе, я подразумеваю, что после появления первого экземпляра производство может выйти на поток.)

Изложу некоторые свои "сырые" умозрительные соображения на основе переписок по этой теме, и дам ссылок на почитать. Вероятно, впоследствии чем-то дополню или поправлю, потому что я плохо разбираюсь в теме и почти ни на кого здесь не опирался.

### Содержание

1\. **[О чем идет речь](414.html#1)** <br>
2\. **[Состоится ли разработка](414.html#2)** <br>
3\. **[Роль ценностей](414.html#3)** <br>
3.1. **[Как все может пойти плохо и очень плохо](414.html#3.1)** <br>
3.2. **[Отличит ли искусственный интеллект хорошее от плохого?](414.html#3.2)** <br>
4\. **[Ход разработки](414.html#4)** <br>
4.1. **[Одобренная разработка](414.html#4.1)** <br>
4.2. **[Неодобренная разработка](414.html#4.2)** <br>
5\. **[Варианты вмешательства](414.html#5)** <br>
5.1. **[Предотвращение](414.html#5.1)** <br>
5.2. **[Участие](414.html#5.2)** <br>
5.3. **[Перехват](414.html#5.3)** <br>
6\. **[Похожая ситуация с человеком](414.html#6)** <br>
**[Что почитать](414.html#read)** <br>

<a name="1"></a>
## 1. О чем идет речь

[Сильный искусственный интеллект](https://ru.wikipedia.org/wiki/Сильный_и_слабый_искусственные_интеллекты) (СИИ) — это гипотетический вид искусственного интеллекта, способный понимать и обучаться любым интеллектуальным задачам, которые способны решать люди. Часть источников также используют этот термин для описания ИИ, способного к сентиентности и самосознанию.

В противоположность сильному, слабым искусственным интеллектом называют те виды искусственного интеллекта, которые запрограммированы на решение наперед определенного узкого класса задач.

Некоторые считают, что создание СИИ может привести к [технологической сингулярности](https://ru.wikipedia.org/wiki/Технологическая_сингулярность), чрезвычайно поменяв наш мир.

<a name="2"></a>
## 2. Состоится ли разработка

Можно ограничиться хотя бы ближайшими пятью столетиями и спросить — возможно ли, что в этот период будет создан СИИ? Я не готов подробно разбирать эту тему и приведу лишь некоторые из аргументов «за»:

1) В нашем мире множество проблем, с которыми люди справляются с трудом или вообще не могут справиться из-за ограничений в вычислительных ресурсах, времени и точности — поэтому у разных групп людей есть сильная мотивация для создания СИИ.

2) Люди не блещут рациональностью и недооценивают риски, связанные с созданием СИИ. В частности, они могут думать, что из высокого интеллекта автоматически следует корректное (желаемое ими) поведение, значит и бояться якобы нечего.

3) И даже когда люди понимают рискованность этой затеи, они могут торопиться, чтобы не дать создать СИИ идеологическому конкуренту.

В любом случае, даже если вероятность создания СИИ невелика, нужно учитывать, что возможные последствия очень и очень весомы, поэтому эту тему нельзя игнорировать.

Думаю, что СИИ с большой вероятностью состоится рано или поздно, если только для этого нет каких-то принципиальных технических ограничений, непреодолимых людьми вовсе (я думаю, что таких нет), или если не случится какой-то катаклизм, который нас остановит. При этом сложно прикинуть, с какой вероятностью СИИ будет сентиентным.

<a name="3"></a>
## 3. Роль ценностей

Я думаю, можно сказать, что в проект СИИ при разработке в некотором смысле будут "вложены" те ценности, которыми обладают его авторы (разумеется, чем менее они компетентны — тем хуже ИИ будет отражать [даже их собственные ценности](https://lesswrong.ru/549)). И если вы не приложите руку к проекту — возможно, что ваших ценностей (таких как важность предотвращения страданий) в полученном ИИ окажется меньше, и результат вас не устроит.

Почему может быть столь критичной разница в ценностях между теми, кто участвует в проекте, и остальными?

<a name="3.1"></a>
### 3.1. Как все может пойти плохо и очень плохо

Как верно было замечено в одной написанной антинаталистами [статье](https://vk.com/wall-199052526_212), «едва ли кому-то хотелось бы жить в мире, где пронатализм господствовал бы безраздельно», т. к. есть много пугающих сценариев развития будущего, которые, тем не менее, обоснованы в рамках тех или иных этических систем, и если соответствующие этические взгляды унаследует могущественный ИИ — мы можем с какой-то вероятностью ожидать реализации этих антиутопических сценариев.

Один из классических примеров — это так называемое «отвратительное следствие» (англ. [repugnant conclusion](https://plato.stanford.edu/entries/repugnant-conclusion/)) в классическом утилитаризме, согласно которому морально правильным будет неограниченное увеличение популяции живых существ с одновременным снижением качества их жизни, пока оно остается выше нуля (что в классическом утилитаризме понимается как превышение объемов удовольствий над объемами страданий), и пока суммарная ценность популяции (общий объем удовольствий за вычетом общего объема страданий) увеличивается.

А в качестве одного из наиболее пугающих примеров можно рассмотреть _панбиотическую этику_, которая призывает просто [заполнять вселенную живыми существами](https://vk.com/wall-199052526_222) (напр., распространяя всюду корабли-терраформеры с «семенами» жизни), т. к. жизнь, согласно этой этике, есть нечто самоценное и прекрасное само по себе (и чем ее больше — тем лучше), даже если она интенсивно страдает.

Сценарии, в которых порождаются объемы страданий, многократно превышающие все, что уже было перенесено населением нашей планеты, называются сценариями [s-рисков](https://centerforreducingsuffering.org/research/intro/) (s — от англ. suffering, страдание).

<a name="3.2"></a>
### 3.2. Отличит ли искусственный интеллект хорошее от плохого?

Конечно, есть те, кто верит в существование объективной и познаваемой морали, и считают, что такая мораль обязательно не одобрила бы вышеописанные сценарии будущего. Они могут думать, что в отличие от многих людей СИИ будет достаточно умен, чтобы отличить хорошее от плохого, и будет заниматься уменьшением страданий, а не их увеличением (в частности, антинаталисты могут думать, что СИИ [стал бы антинаталистом](https://www.edge.org/conversation/thomas_metzinger-benevolent-artificial-anti-natalism-baan)). Есть ряд проблем с этим взглядом.

Во-первых, вы можете думать, что ваши взгляды отражают общезначимые ценности, но упускать, что у людей есть множество способов выставлять приоритеты между этими ценностями, и получать совершенно разные этические системы в результате. Например, если никто не хочет страдать, следует ли из этого, что предотвращение страданий в любых обстоятельствах и любом направлении является благом? Нет, не следует. Так, если самоценность жизни или каких-то ее элементов для кого-то будет достаточно велика, то производство даже довольно несчастной жизни может выглядеть для него благом — выгода просто перевесит убытки. Если вы собираетесь (как нередко делают антинаталисты) апеллировать к аргументу о том, что никто не дает согласия на свое рождение, то даже если другие согласятся с вами, отрицательная ценность этого момента может быть недостаточно значительной для них. Кроме того, вам нужно разобраться, какой из нормативных моральных подходов верен (_см. раздел о нормативной этике в [моей статье](436.html)_), и почему вы считаете, что все остальные ошибаются.

Во-вторых, стоило бы рассмотреть гипотезу о том, что мораль как явление вообще не существует за пределами наших умов и культур. Возможно, что наши представления о хорошем и плохом — лишь результат естественных процессов отбора (генетического и/или социокультурного), или в части случаев даже нечто побочное (мутация, случайное отклонение) по отношению к основному направлению отбора (_так, если вы бенатарианец — вы сторонник менее выигрышного вида морали в генетическом плане, т. к. отказываетесь размножаться; кроме того, ваш вид морали менее выигрышен и в культурном плане, т. к. воспринимается большинством людей как слишком мрачный, недостаточно воодушевляющий_). Стоит также рассмотреть возможность того, что моральные высказывания сами по себе не истинны и не ложны, а лишь выражают чувства или предпочтения говорящих (_загляните в раздел о метаэтике из [моей статьи](436.html)_). Означает ли отсутствие объективной морали, что «все дозволено»? Я так не считаю. От того, что мое желание уменьшать именно (интенсивные) страдания во вселенной — субъективно, оно никуда не денется (см. «[Что вы сделаете без морали?](https://lesswrong.ru/658)» Юдковского, а также [объяснение](https://as-merlin.livejournal.com/86976.html), почему из морального релятивизма не следует ни необходимости толерантности к враждебным взглядам, ни бездействия). Просто мне придется признать, что мои моральные предпочтения могут не разделяться другими людьми в той же мере. И при этом все еще будет немало прикладных моральных принципов, полезных просто в силу того, что они позволяют даже людям с разными ценностями достигать наибольшего выигрыша [в результате кооперации](https://longtermrisk.org/gains-from-trade-through-compromise/).

В-третьих, стоит рассмотреть возможность того, что даже если объективная мораль в каком-то смысле существует, ее познание намного сложнее, чем нам кажется (и может быть, большинство людей, включая вас, вообще ошибаются в своих моральных представлениях). Возможно, мы и не представляем, что именно должен изучать СИИ, чтобы открыть эту самую «объективную мораль», чем бы она ни была. Не будучи способными верно определить направление поиска, не имея нужных критериев — отчего же мы должны быть уверены, что у СИИ все получится? Что, если моральное познание чем-то принципиально отличается от иных форм познания, и высокий интеллект, в отличие от глубокого опыта бытия человеком, не поможет в этом деле? Быть может, верные моральные представления — продукт специфических человеческих практик (просветления?), и первые поколения СИИ просто не будут иметь технического оснащения, которое требуется для подобного морального познания.

Но даже оставив споры о природе и познаваемости морали в стороне, вам стоит рассмотреть несколько возможностей того, как все может пойти не так:

1.  СИИ может быть недостаточно умным в целом, однако достаточно влиятельным — напр., если его производство будет поставлено на поток и в мире будет достаточно много его экземпляров и/или ему по ошибке доверят большую власть;
2.  СИИ может быть неравномерно компетентным — напр., прекрасно решать задачи в области химии и физики, но плохо понимать вопросы морали из-за недостаточно адаптированной к моральному мышлению конструкции (может быть, из-за чисто языковых барьеров между ним и его создателями, а может потому, что создатели и сами не очень-то разбираются в морали);
3.  СИИ может быть тем или иным путем жестко подчинен неким начальным директивам и не способен их переписать или перечить им — напр., в него может быть встроено отдельное, неконтролируемое им устройство, которое при любой попытке отойти от директив заставляет его испытывать ужасные мучения, отключает его (чего он, может быть, стремится избежать), или стирает ему память и перезагружает.

Вы вполне можете представить похожие сценарии в отношении людей: люди могут быть не слишком умными, но иметь влияние (из-за своего положения и/или многочисленности); люди могут быть гениальными в чем-то одном, но некомпетентными в другом; люди могут быть порабощены и принуждаемы, на них можно надеть устройство со взрывчаткой, ограничивающее свободу их передвижения, и т. п.

<a name="4"></a>
## 4. Ход разработки

Как может в целом пойти ход разработки СИИ, если она состоится? Я вижу по крайней мере два крайних сценария — широко одобренную и неодобренную разработку.

<a name="4.1"></a>
### 4.1. Одобренная разработка

Если достаточно широкая группа агентов кооперативно настроены друг к другу и готовы сообща работать над проектом, он может быть официальной, международно признанной и курируемой разработкой.

Положительный момент состоит в том, что благодаря прозрачности и совместности, проект будет включать в себя больше разных ценностей и будет более безопасным.

Отрицательный момент состоит в том, что проект осуществится с большей вероятностью, будет более масштабным и может принести больший ущерб (больше страданий).

<a name="4.2"></a>
### 4.2. Неодобренная разработка

Если у проекта лишь малое число убежденных сторонников, но они имеют достаточно много ресурсов, а остальная общественность недостаточно кооперативно настроена к ним, возможно, что разработка будет идти незаконно; или, если сторонники проекта где-то политически и/или географически локализованы — он может быть реализован какой-то отдельной страной или организацией при неодобрении со стороны остальных.

Отрицательный момент состоит в том, что результат может отражать ценности узкой группы лиц, быть попросту небезопасным или сильно отклониться от задумки авторов из-за недостаточной компетентности и из-за желания осуществить проект как можно скорее (ведь иначе обгонят конкуренты).

Положительный момент состоит в том, что вероятность осуществления проекта будет низкой, да и масштабы (и возможный ущерб) будут скорее всего небольшими, если только разработчиком не выступала целая страна.

<a name="5"></a>
## 5. Варианты вмешательства

Итак, какие варианты есть у сторонников уменьшения страданий?

Я вижу три основных возможных подхода: предотвращение, участие, перехват.

<a name="5.1"></a>
### 5.1. Предотвращение

Я думаю, что даже если все сторонники уменьшения страданий сфокусируются на попытках бескомпромиссно предотвратить создание СИИ, в то время как общество в целом будет склоняться к одобрению разработки, у них почти наверняка ничего не выйдет, т. к. их довольно мало (в частности, крайне мало бенатарианцев — и вряд ли когда-нибудь их будет много, в чем уверен и сам Бенатар).

Не буду отрицать, что протесты могут повлиять на содержание проекта, заставив его создателей больше считаться с ценностями протестующих. Правда, протестующие-то могут быть довольно разными помимо сторонников уменьшения страданий, и делить пространство протеста с частью из них может оказаться невыгодным. Кроме того, если протестующие слишком агрессивны, например, то они могут даже отталкивать авторов проекта от своих ценностей. 

С другой стороны, если общество еще не готово одобрить разработку СИИ, протестующие могут помочь предотвратить неодобренные обществом разработки и оттянуть ту, которая все же состоится. Однако, это уже что-то на грани со следующим подходом.

<a name="5.2"></a>
### 5.2. Участие

Думаю, если часть сторонников уменьшения страданий примут участие в работе над общим, одобренным проектом, они получат больше возможностей вложить в него свои ценности, чем при попытках остановить его.

Они могут оказать больше влияния и будучи **_умеренными_** противниками проекта, призывая всех, например, подождать и изучить вопрос поглубже — как из соображений о возможной сентиентности СИИ (как это [делает](https://vk.com/wall-194967191_292) Метцингер), так и из соображений безопасности СИИ.

Благодаря успешному вкладу ценностей в проект и/или продлению работы над ним, последствия дальнейшей деятельности СИИ могут быть менее вредоносными (может быть, он даже уменьшит сумму будущих страданий в мире), да и сам он будет страдать меньше или с меньшей вероятностью (в случае его сентиентности).

Сверх уже упоминавшихся [выше](414.html#3.1) рисков больших страданий, которые удастся предотвратить, положительными последствиями создания СИИ могут быть и такие:

1.  Создание все более эффективных обезболивающих;
2.  Нахождение лекарств от болезней;
3.  Генная инженерия будущих детей, избавляющая их от врожденных патологий и повышенных рисков плохой жизни;
4.  Остановка старости, ослабление влияния смертности, и снижение потребности в воспроизводстве;
5.  Предотвращение конфликтов и войн благодаря повышению связанности человечества, развитию взаимопонимания и сострадания;
6.  Реализация [аболиционистского проекта](https://vk.com/wall-199052526_89) в трансгуманизме — такой перестройки людей и иных животных, которая исключила бы страдания из их жизни (или радикально уменьшила бы их);
7.  Добровольное вымирание, если вы верите, что достаточно разумный СИИ когда-нибудь [откроет для всех антинатализм](https://www.edge.org/conversation/thomas_metzinger-benevolent-artificial-anti-natalism-baan);
8.  Нахождение решения для проблемы [страданий в дикой природе](https://vk.com/wall-199052526_67), на иных планетах, в будущих эпохах или иных вселенных.

<a name="5.3"></a>
### 5.3. Перехват

Последний вариант, который кажется мне совершенно безнадежным, мог бы привлечь некоторых из наименее кооперабельных и наиболее фанатичных сторонников уменьшения страданий — таких, которые хотели бы не препятствовать созданию СИИ, а создать его единолично, не делая уступок другим ценностям (возможно, даже опередив остальных), либо незаконно захватить уже созданный СИИ и использовать его в своих целях.

Возможной целью их проекта может оказаться максимально грубая, быстрая, экстремистская попытка уменьшить страдания в больших масштабах (напр., путем запуска некой «[машины судного дня](https://ru.wikipedia.org/wiki/Машина_Судного_дня)»). Экстремистский характер такого проекта резко ограничивает возможность кооперации, а следовательно и ресурсы на осуществление идеи. А из-за недостаточного числа компетентных создателей и из-за их поспешности результат может сработать совсем не так, как они этого хотели бы. 

Попытка сделать из инструментов разрушения то, что уменьшит страдания — подобна поиску иголки в стоге сена. Чуть отклонившись, ищущие получат прямо противоположное тому, чего желали. Возможны разные варианты того, как все пойдет наперекосяк:

1. Если человечество не исчезнет, то произошедшее может вызвать ненависть общественности к сторонникам уменьшения страданий (как морального приоритета), что существенно ослабит их влияние и способность уменьшать страдания в будущем; в частности, резко возросший спрос на жизнеутверждающие ценности может породить антиутопическое будущее, в котором (пост)люди всеми силами стремятся сохранить и распространить жизнь по вселенной, не уделяя достаточно внимания ее благополучию;
2. Если исчезнет человечество, но не вся жизнь на планете, то свободная от влияния людей и [полная разнообразных страданий](https://vk.com/wall-199052526_67) дикая природа просуществует еще много миллионов лет, прежде чем расширяющееся солнце сожжет поверхность планеты; возможно, появятся даже новые формы разумной жизни и тоже будут страдать;
3. Если же исчезнет все живое на планете, то вполне возможно, что плотность населения неразвитой жизнью во вселенной за пределами нашей планеты была или будет в будущем довольно велика, но нами уже будет упущена возможность руками развитого (пост)человечества уменьшить огромные объемы страданий во вселенной;
4. И даже если исчезнет вся жизнь в досягаемой вселенной, может оказаться, что упущена возможность повлиять на более глобальные условия, которые обуславливают, сколько наполненных страданиями вселенных будут существовать, или будет ли данная вселенная циклически воспроизводить галактики, звезды, планеты, жизнь и страдания снова и снова.

Конечно, вероятность осуществления подобного экстремистского проекта низка. Но достаточно уже и сейчас даже выразить согласие с такой идеей, чтобы вызвать ненависть и подозрения к себе и другим (более кооперабельным) сторонникам уменьшения страданий.

Насколько мне известно, этот экстремисткий подход осуждается всеми [эффективными альтруистами](https://vk.com/wall-199052526_78) с негативно-утилитарными взглядами (такими как Брайан Томасик, Дэвид Пирс, Магнус Виндинг и др.). Интересует он лишь некоторых маргиналов с (на первый взгляд) похожими на негативный утилитаризм или антинатализм взглядами, но весьма слабой теоретической подготовкой.

<a name="6"></a>
## 6. Похожая ситуация с человеком

Рассмотрим также в чем-то похожий случай с рождением человека.

Допустим, ваши братья и сестры хотят завести детей. Отговорить вы их, скорее всего, не сможете. Но сможете повлиять на ребенка, если вызоветесь участвовать в воспитании.

Родственники знают, что вы не одобряете размножение (по крайней мере в случае, если вы согласны со строгой антинаталистической позицией Дэвида Бенатара в отношении людей). Но возможно, если вы не слишком отталкивающе себя ведете, они уважают вашу чуткость к страданиям и согласны хотя бы чуть-чуть сместить свои взгляды в вашу сторону.

В частности, вы могли бы рекомендовать беременным родственникам обследование на ранней стадии и аборт в случае обнаружения тяжких проблем. А может быть, вы могли бы уговорить их применить генную инженерию или [генную терапию](https://biomolecula.ru/articles/gennaia-terapiia-poznakomtes-s-lekarstvami-budushchego) (если в вашей стране это уже практикуется). Так или иначе, ваше участие может предотвратить хотя бы самые плохие из возможных вариантов жизни будущего ребенка (наследственные болезни, врожденные патологии, повышенную склонность к раку и т. п.). 

Далее, если вы достаточно расположены к общению с ребенком, родственники, возможно, могли бы доверить вам часть его воспитания, и у вас была бы возможность общаться с ним и учить чему-то хорошему. Так, вы могли бы попробовать воспитать его так, чтобы он не причинял вреда тому, что вам дорого (например, не стал бы без веских причин плодить животных). Но ведь если он даже просто вырастет с большей чуткостью к страданиям и поможет найти, к примеру, лекарство от рака — это будет тоже очень неплохим результатом? С другой стороны, если вы всех оттолкнете от себя, шансы повлиять на этого ребенка значительно уменьшатся.

Так что же лучше — принять решение ваших родственников как факт и смягчить его последствия, или остаться в гордом одиночестве, порвав с ними и не имея больше возможности положительно влиять на происходящее?

<a name="read"></a>
## Что почитать

**О морали, ее разновидностях и подходах:**

1. [Что такое этика?](436.html), К. Кирдан;
2. [Введение в эффективный альтруизм](https://vk.com/wall-199052526_78), ЭА;
3. [Что такое негативный утилитаризм](435.html), К. Кирдан;
4. [О российских антинаталистах](https://vk.com/wall-199052526_158), интервью на MK.RU;
5. [Антинатализм, что ты такое? Антинаталисты, чего вы хотите?](https://vk.com/wall-199052526_212), Антинатализм адекватного человека;
6. [Что вы сделаете без морали?](https://lesswrong.ru/658), Элиезер Юдковский;
7. [Этический релятивизм](https://as-merlin.livejournal.com/86976.html), Алексей Ерпылев.

**О том, что такое сентиентность:**

1. [«Cентиентность — недостающее слово в нашем языке»](466.html), К. Кирдан;
2. [Sentience](https://www.animal-ethics.org/sentience-section/), Animal Ethics;
3. [Sentience](https://sentience-research.org/sentience/), Sentience Research;
4. [Сентиентность](https://vk.com/wall-199052526_183), Иван Попов — у автора несколько ограниченная точка зрения, но суть понятия здесь можно уловить.

**О проблеме возможной сентиентности СИИ:**

1. [The Ethics of Artificial Intelligence](https://intelligence.org/files/EthicsofAI.pdf), Nick Bostrom & Eliezer Yudkowsky;
2. [Наука о мозге и миф о своем Я. Тоннель Эго](https://batrachos.com/sites/default/files/pictures/Books/Mettsinger_2016_Nauka o mozge i mif o svoem ya.pdf) (Глава 7. Искусственные эго-машины), Томас Метцингер;
3. [Искусственное страдание: Аргумент в пользу глобального моратория на синтетическую феноменологию](https://vk.com/wall-194967191_1276), Томас Метцингер.

**О проблеме возможных ужасных последствий изобретения СИИ:**

1. [Почему стоит работать над вопросами безопасности искусственного интеллекта уже сейчас](https://vk.com/wall-199052526_318), ЭА;
2. [Altruists Should Prioritize Artificial Intelligence](https://longtermrisk.org/altruists-should-prioritize-artificial-intelligence/), Lukas Gloor;
3. [Artificial Intelligence and Its Implications for Future Suffering](https://longtermrisk.org/artificial-intelligence-and-its-implications-for-future-suffering), Brian Tomasik;
4. [S-risks: An introduction](https://centerforreducingsuffering.org/research/intro/), Tobias Baumann;
5. [The Repugnant Conclusion](https://plato.stanford.edu/entries/repugnant-conclusion/), Stanford Encyclopedia of Philosophy;
6. [Умножит ли колонизация космоса страдания диких животных](https://vk.com/wall-199052526_222), Брайан Томасик.

**О сложности сотворения СИИ с нужными создателям ценностями:**

1. [Скрытая сложность желаний](https://lesswrong.ru/549), Элиезер Юдковский;
2. [Тезис об ортогональности](https://vk.com/wall-199052526_338), Ник Бостром;
3. [Мысленный эксперимент «максимизатор скрепок»](https://i-future.livejournal.com/657861.html);
4. [Навстречу сингулярности](https://intelligenceexplosion.com/ru), Люк Мюлхаузер.

**О преимуществах кооперации и миролюбия в деле уменьшения страданий:**

1. [О пользе любезности, общин и цивилизации](https://lesswrong.ru/281), Скотт Александер;
2. [Why altruists should be cooperative](https://centerforreducingsuffering.org/why-altruists-should-be-cooperative/), Magnus Vinding;
3. [Gains from Trade through Compromise](https://longtermrisk.org/gains-from-trade-through-compromise/), Brian Tomasik;
4. [Reasons to Be Nice to Other Value Systems](https://longtermrisk.org/reasons-to-be-nice-to-other-value-systems/), Brian Tomasik.

**О возможностях глобального уменьшения страданий:**

1. [Гедонистический императив](https://vk.com/wall-199052526_89), Дэвид Пирс;
2. [Должны ли мы вмешиваться в природу?](https://vk.com/wall-199052526_67), Брайан Томасик.

**Об особом сценарии — может ли сам СИИ быть антинаталистом:**

1. [Benevolent Artificial Anti-Natalism (BAAN)](https://www.edge.org/conversation/thomas_metzinger-benevolent-artificial-anti-natalism-baan), Thomas Metzinger;
2. [A reply to Thomas Metzinger’s BAAN thought experiment](https://longtermrisk.org/reply-thomas-metzingers-baan-thought-experiment/), Lukas Gloor.