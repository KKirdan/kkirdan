---
layout: arbital
title: Согласование ИИ
author: "Элиезер Юдковский"
original_title: AI alignment
original_date: 2015.03.27
original: https://arbital.com/p/ai_alignment/
original2: https://arbital.greaterwrong.com/p/ai_alignment
translated_by: К. Кирдан
translated_when: 2024.05.16
license:
  - CC BY 3.0
  - https://creativecommons.org/licenses/by/3.0/deed.ru
excerpt: "«Проблема согласования продвинутых агентов» или «согласование ИИ» — это главная тема в исследованиях того, как разработать достаточно продвинутый машинный интеллект так, чтобы его использование приводило к хорошим последствиям в реальном мире."
---
«Проблема согласования [продвинутых агентов](https://arbital.com/p/advanced_agent/)» или «согласование ИИ» — это главная тема в исследованиях того, как разработать [достаточно продвинутый машинный интеллект](https://arbital.com/p/sufficiently_advanced_ai/) так, чтобы его использование приводило к [хорошим](https://arbital.com/p/beneficial/) последствиям в реальном мире.

И «[продвинутых агентов](https://arbital.com/p/advanced_agent/)», и «[хорошее](https://arbital.com/p/value_alignment_value/)» следует понимать как метасинтаксические заполнители, соответствующие сложным идеям, которые все еще остаются предметом дискуссий. Термин «согласование» призван передать идею направления ИИ в определенную сторону — подобно направлению в определенную сторону ракеты, которую вы построили.

«Теория согласования ИИ» задумана как объемлющий термин, охватывающий всю область исследований, связанную с этой проблемой, включая, например, широко обсуждаемые попытки оценить, насколько быстро ИИ сможет расширить свои возможности по мере преодоления разных конкретных порогов.

Другие термины, которые использовались для описания этой исследовательской проблемы, включают «[надежный](https://arbital.com/p/AI_safety_mindset/) и [полезный](https://arbital.com/p/beneficial/) ИИ» и «Дружественный ИИ». Термин «[проблема согласования ценностей](https://arbital.com/p/value_alignment_problem/)» был введен Стюартом Расселом для обозначения основной подзадачи согласования предпочтений ИИ с (потенциально [идеализированными](https://arbital.com/p/cev/)) человеческими предпочтениями.

Некоторые альтернативные термины для этой общей области исследований, такие как «проблема контроля», могут звучать [враждебно](https://arbital.com/p/nonadversarial/) — как будто ракета уже направлена ​​в неправильном направлении, и вам нужно с ней бороться. Другие термины, такие как «безопасность ИИ», недооценивают предполагаемую степень того, как согласованность должна быть неотъемлемой частью создания продвинутых агентов. Например, не существует отдельной теории «безопасности мостов» о том, как строить мосты, которые не рушатся. Направление агента в определенную сторону следует рассматривать как часть стандартной задачи создания продвинутого машинного агента. Проблема не делится на «создание продвинутого ИИ», и затем отдельно на «каким-то образом заставить этот ИИ давать хорошие последствия». Проблема в том, чтобы «получить хорошие последствия посредством создания когнитивного агента, который приводит к этим хорошим последствиям».

Хорошей вводной статьи или обзорной статьи по этой области в настоящее время нет. Если вы понятия не имеете, в чем заключается эта проблема, подумайте о том, чтобы почитать [популярную книгу Ника Бострома «Искусственный интеллект»](https://arbital.com/p/bostrom_superintelligence/)<sup>*</sup>.

Вы можете изучить на Arbital данную область, перейдя по [этой ссылке](http://arbital.com/explore/ai_alignment). См. также "[Список тем по согласованию ценностей](https://arbital.com/p/value_alignment_subject_list/)" на Arbital, хотя он уже неактуален.

---

\*\. Книга переведена на русский язык и [издана](https://www.mann-ivanov-ferber.ru/books/iskusstvennyj-intellekt/) «Манн, Иванов и Фербер» — прим. пер.
